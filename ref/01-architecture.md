# llm-info アーキテクチャリファレンス

## プロジェクト概要

llm-infoは、LLMゲートウェイからモデル情報を取得して表示するCLIツールです。

### 主な特徴

- LiteLLM互換エンドポイントとOpenAI標準エンドポイントの両対応
- 自動フォールバック機能
- 設定ファイル、環境変数、コマンドライン引数による柔軟な設定管理
- テーブル/JSON形式での出力
- 高度なフィルタリング・ソート機能
- 詳細なエラーハンドリングと解決策の提示

## プロジェクト構造

```
llm-info/
├── cmd/llm-info/          # エントリーポイント
│   ├── main.go            # メイン処理
│   └── help.go            # ヘルプシステム
├── internal/              # 内部パッケージ
│   ├── api/              # API通信層
│   ├── config/           # 設定管理
│   ├── error/            # エラーハンドリング
│   ├── model/            # データモデル
│   └── ui/               # UI出力層
├── pkg/config/           # 公開設定インターフェース
├── test/                 # テストコード
│   ├── e2e/             # E2Eテスト
│   └── integration/     # 統合テスト
└── plans/               # 設計ドキュメント
```

## アーキテクチャレイヤー

### 1. コマンドライン層 (cmd/)

**責務**: ユーザーインターフェース、引数解析、実行フロー制御

**主要コンポーネント**:
- `main.go`: アプリケーションのエントリーポイント
- `help.go`: ヘルプシステムとドキュメント表示

**フロー**:
1. コマンドライン引数の解析
2. 設定の読み込みと解決
3. APIクライアントの初期化
4. モデル情報の取得
5. 結果の表示

### 2. API通信層 (internal/api/)

**責務**: LLMゲートウェイとの通信、エンドポイントの管理

**主要コンポーネント**:
- `client.go`: LiteLLM互換エンドポイント (`/model/info`) のクライアント
- `standard_client.go`: OpenAI標準エンドポイント (`/v1/models`) のクライアント
- `endpoints.go`: エンドポイント定義
- `response.go`: レスポンス構造体

**重要な機能**:
- `FetchModelsWithFallback()`: 自動フォールバック機能の実装
  1. OpenAI標準エンドポイントを優先試行
  2. 成功時、LiteLLMで詳細情報の追加取得を試行
  3. 標準失敗時、LiteLLMにフォールバック

### 3. 設定管理層 (internal/config/)

**責務**: 設定の読み込み、優先順位の解決、検証

**主要コンポーネント**:
- `manager.go`: 設定マネージャー（統合管理）
- `file.go`: YAML設定ファイルの読み書き
- `env.go`: 環境変数からの設定読み込み
- `validator.go`: 設定の検証

**設定の優先順位** (高→低):
1. コマンドライン引数
2. 環境変数
3. 設定ファイル
4. デフォルト値

### 4. エラーハンドリング層 (internal/error/)

**責務**: エラーの分類、メッセージ生成、解決策の提示

**主要コンポーネント**:
- `handler.go`: エラーハンドラー
- `messages.go`: エラーメッセージの生成とエラー検出
- `solutions.go`: エラーごとの解決策定義

**エラー分類**:
- `ErrorTypeNetwork`: ネットワーク関連
- `ErrorTypeAPI`: API関連
- `ErrorTypeConfig`: 設定関連
- `ErrorTypeUser`: ユーザー入力関連
- `ErrorTypeSystem`: システム関連

### 5. データモデル層 (internal/model/)

**責務**: アプリケーション内のデータ構造定義

**主要構造体**:
```go
type Model struct {
    Name      string  // モデル名
    MaxTokens int     // 最大トークン数
    Mode      string  // モード（chat等）
    InputCost float64 // 入力コスト
}
```

### 6. UI出力層 (internal/ui/)

**責務**: データの整形と表示

**主要コンポーネント**:
- `table.go`: テーブル形式表示
- `json.go`: JSON形式出力
- `filter.go`: モデルフィルタリング
- `sort.go`: モデルソート
- `columns.go`: カラム管理

**機能**:
- 動的列制御（データの有無に応じて列を表示/非表示）
- 高度なフィルタリング（`name:gpt`, `tokens>100000`等）
- 複数条件ソート

## データフロー

```
ユーザー入力
    ↓
[コマンドライン層]
    ↓
設定解決 → [設定管理層]
    ↓
[API通信層]
    ├→ OpenAI標準エンドポイント試行
    │   ├→ 成功 → LiteLLM詳細情報試行
    │   │   ├→ 成功 → 詳細情報を返す
    │   │   └→ 失敗 → 基本情報を返す
    │   └→ 失敗 → LiteLLMフォールバック
    │       ├→ 成功 → 詳細情報を返す
    │       └→ 失敗 → エラー
    ↓
[データモデル層] モデル情報
    ↓
[UI出力層]
    ├→ フィルタリング
    ├→ ソート
    └→ 表示（テーブル/JSON）
    ↓
出力表示
```

## 重要な設計判断

### 1. フォールバック順序の逆転 (2026-01-10)

**理由**:
- OpenAI標準エンドポイントの方が広く実装されている
- 最初の試行での成功率向上
- 警告メッセージの削減

**実装**:
`internal/api/client.go:120-150` の `FetchModelsWithFallback()`

### 2. 設定の階層管理

**理由**:
- 柔軟な設定方法の提供
- 環境に応じた使い分け

**実装**:
`internal/config/manager.go` の `ResolveConfig()`

### 3. 詳細なエラーハンドリング

**理由**:
- ユーザーへの明確なフィードバック
- 問題解決の迅速化

**実装**:
`internal/error/` パッケージ全体

## テスト戦略

### ユニットテスト
- 各パッケージに `*_test.go` ファイル
- モックサーバーを使用したAPI通信のテスト

### 統合テスト
- `test/integration/` ディレクトリ
- フォールバック、設定管理、エラーハンドリングのテスト

### E2Eテスト
- `test/e2e/` ディレクトリ
- 実際の使用シナリオのテスト

## 拡張ポイント

### 新しいエンドポイントの追加

1. `internal/api/` に新しいクライアント実装を追加
2. `FetchModelsWithFallback()` にフォールバックロジックを追加
3. `internal/api/endpoints.go` にエンドポイント定義を追加

### 新しい出力形式の追加

1. `internal/ui/` に新しいレンダラーを追加
2. `cmd/llm-info/main.go` の出力形式分岐に追加

### 新しいフィルタリング条件の追加

1. `internal/ui/filter.go` にフィルタロジックを追加
2. フィルタ構文を拡張

## 依存関係

### 外部ライブラリ
- `gopkg.in/yaml.v3`: YAML設定ファイルの読み書き

### 標準ライブラリのみ
- `net/http`: HTTP通信
- `encoding/json`: JSONパース
- `flag`: コマンドライン引数解析
- `time`: タイムアウト管理

## パフォーマンス考慮事項

### 二重リクエスト
OpenAI標準成功後もLiteLLMを試行するため、レスポンス時間が最大2倍になる可能性があります。

**軽減策**:
- タイムアウトを適切に設定（デフォルト10秒）
- 将来的に `--skip-enhanced-details` フラグの追加を検討

### メモリ使用
モデル数が多い場合、全データをメモリに保持します。

**現状**: 問題なし（通常は数百モデル程度）
**将来的な改善**: ストリーミング処理の検討

## セキュリティ考慮事項

### APIキーの管理
- コマンドライン引数での直接指定は推奨しない
- 環境変数または設定ファイルを使用
- 設定ファイルのパーミッションを適切に設定（0600推奨）

### HTTPSの使用
- URLスキームの検証を実施
- HTTPSの使用を推奨（強制はしない）

## 関連ドキュメント

- [API通信リファレンス](02-api.md)
- [設定管理リファレンス](03-config.md)
- [エラーハンドリングリファレンス](04-error.md)
- [UI出力リファレンス](05-ui.md)
